学习方法：

	1. 根据任务要求确定学习算法模型；
	2. 划分训练集、验证集和测试集，训练集用来训练模型，验证集用来调整超参数，测试集用来评估性能；
	3. 数据集归一化；
	4. 选定初始超参数；
	5. 以最小化训练误差为原则执行训练；
	6. 执行验证并调整超参数；
	7. 执行测试并根据测试误差优化模型；
	8. 度量模型性能；


机器学习模型优劣因素：（量化）

	1. 降低训练误差。
	2. 缩小训练误差和测试误差的差距。



在特定数据集中寻找相对优秀的模型的方法：

	1. 调整假设空间，限制数据集的表示容量。
	2. 正则化


小规模数据集条件下可以采用交叉验证来产生泛化误差的准确估计。例如， K-折交叉验证算法。

参数学习的做法是希望以指定数据集为基础在假设空间中寻找一个最好的函数来拟合，拟合后再做预测，但是通常是这个最好的函数也是人为主观指定的，需要寻找一种方法来客观的完成这个任务。函数估计是一种可行的办法。通常采用均方误差来描述函数估计结果和真实函数间的误差。最小化均方误差则可以调整函数估计结果，使其更接近真实函数。

也即，使用函数估计方法来估计需要拟合哪一种里的哪一个函数，然后用交叉熵表示和量化泛化误差。

函数估计方法：

	1. 极大似然估计。真实参数是未知定值，而点估计是考虑数据集上函数的随机变量。当样本数目小到会发生过拟合时，正则化策略可以用于获得训练数据有限时方差较小的极大似然有偏版本。
	2. 贝叶斯统计。认为真实参数是随机变量，数据集是直接观测结果，不随机。


泛化误差表示方法：

	1. 均方误差
	2. KL散度


分类算法：

	1. 监督学习算法：

          a. 概率监督学习，逻辑回归。在回归之后使用非线性函数转换值域，然后根据值域值表示分类。
          b. 支持向量机。使用核方法做函数预测，将线性函数替换为核函数表示法。可以将非线性问题转换为线性问题，然后但问题是，怎样选择和选择什么样的核函数。
          c. 最近邻回归。一种非参数学习算法
          d. 决策树。


	1. 无监督学习算法：

          a. 主成分分析（PCA），一种将数据变换为元素间彼此不相关的能力，是消除数据中未知变化因素的简单表示示例。这个消除是通过寻找输入空间的一个旋转，使得方差的主坐标和新变换出的空间基对齐。也是一种降维手段。
          b. K-均值聚类，每个训练样本分配到最近的中心点所代表的聚类，然后每个中心点更新为聚类中所有样本的均值，而后重复上述操作知道最后中心点更新接近停止。


几乎所有深度学习算法都可以分解为：特定数据集、代价函数、优化过程和模型。

深度学习的问题：

	1. 维数灾难
	2. 局部不变性和平滑正则化，机器学习需要由先验知识去引导应该学习什么类型的函数，这种先验知识包括平滑先验或者局部不变性先验，这样表明学习的函数不应再小区域内发生很大变化。这样的先验不足以应对人工智能任务。需要假设数据？深度的分布式表示带来的指数增益有效地解决了维数灾难带来的挑战。
	3. 流形学习（manifold learning）。流形是指连接在一起的区域，即每个点都有邻接点。为避免在整个实数n维空间中学习，通过假设隐式流形来控制学习方向、减小维度。这个假设的前提是概率质量高度集中，另外一个前提是我们能够非正式地想象这些领域和变换。



在梯度的基础上完成机器学习，目的是寻找基于样本空间所选择参数的最小值或者相对小的值。需要选择合适的代价函数来根据极大似然算法计算最小代价时的参数值。但是样本集所服从的概率密度函数未知，需要事先指定，如指定正态分布。

可以不去学习完整的概率分布，而只学习给定自变量时因变量的某个条件统计量。可以将神经网络看做是可以表达任意函数映射的模型，但是在训练条件下被特征所限制，而后只选择一个函数来表达样本集数据间的关系。因此，可以将代价函数看作是泛函，然后在优化过程中得到想要的某些特殊函数处取得最小值。这种计算需要使用变分法。这样不是指定分布函数，然后求在该分布函数下能得到最小误差的参数（这里的问题是样本集分布函数可能完全不是这个指定的函数，所以即使得到最小误差，这个参数可能完全没有意义），而是将函数看做变量，求能够使样本映射误差最小的分布函数（这个才是更正确的做法，而不是靠猜）。

交叉熵代价函数比均方误差或平均绝对误差更适合采用梯度优化算法。

输出单元的选择：

	1. 线性输出层用来产生条件高斯分布的均值
	2. sigmoid用来输出伯努利分布，即二分类问题。需要用极大似然来训练sigmoid输出单元，而不是梯度下降算法。
	3. softmax用来输出Multinoulli分布，多分类问题。
	4. 通常使用ReLU替换sigmoid来更适合梯度下降算法。



深度学习正则化目的是增加偏差减少方差，这样可以降低泛化误差。
方法：

	1. 参数范数惩罚：

正则化项包括2次项和1次项，2次项使用平方范数，1次项使用一阶范数。
使用平方范数时，正则化的效果是在参数矩阵特征分解后，在特征值对角矩阵上增加数值，也即在特征向量基上缩放，以此调整特征权重。
使用一阶范数时，可以将正则化函数二次近似分解为参数求和形式，而后在不同的情况下会将权重衰减为0或者在特定方向上移动一定距离。一阶范数正则化或产生更稀疏的权重解。这种性质可以应用于特征选择，即从可用的特征子集中选择出有意义的特征，化简机器学习问题。例如LASSO


	1. 作为约束的范数惩罚

使用范数时也可以将各权重分量减去特定数值，以提供范数的约束。


	1. 正则化和欠约束问题

如果在例如线性回归和PCA等过程中需要求自变量矩阵的逆矩阵，但该矩阵是奇异的，可以通过添加正则化项将自变量矩阵转化为非奇异矩阵，以此可以求出对应的伪逆，使得可以求得对应的权重矩阵，进而继续完成优化。


	1. 数据集增强，在真实数据集上人工处理或生成假数据集
	2. 噪声鲁棒性

标签平滑(label soothing)，把确切分类目标从0和1替换成ϵ/k-1 和1-ϵ，正则化具有k个输出的softmax函数模型。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。


	1. 半监督学习
	2. 多任务学习，多个任务共享通用特征参数
	3. 提前终止，当验证集上误差在实现指定的循环次数内没有进一步改善时终止算法。提前终止比权重衰减更具有优势，提前终止能自动确定正则化的正确量，而权重衰减需要多个训练试验测试其超参数的不同值。
	4. 参数绑定和参数共享

参数绑定是使用两个模型执行相同的任务，但输入分布稍微不同，这些任务会足够相似，因此认为模型参数应彼此靠近。这种方式使得许多分类模型中的参数能与之对应的无监督模型参数匹配。
参数共享，强制要求某些参数相等，将各种模型或模型组件解释为共享唯一的一组参数，这种方式可以减少内存使用。


	1. 稀疏表示，随机放弃同一层上的一部分神经元，或者使用一阶范数惩罚，或者其他方式，如正交匹配追踪(OMP)，OMP-1可以成为深度架构中非常有效的特征提取器。
	2. Bagging，模型平均。使用多个模型降低泛化误差的技术。
	3. Boosting。使用多个单个神经网络构建集成网络。
	4. Dropout。

Bagging集成需要根据所有模型的累积做预测，这个过程称为推断。集成的预测是由这些单独的模型的概率分布的算术平均值给出。在dropout的情况下通过各个子模型的概率分布计算平均值。权重比例推断规则(weight scaling inference rule)
批标准化？


	1. 对抗训练，在具有欺骗性的样本上完成训练，用来确认模型是否能达到和人类水平相同的能力。虚拟对抗样本？
	2. 切面距离、正切传播和流形正切分类器



优化方法：
机器学习的优化是尝试降低代价函数来提高模型性能，目的是提高模型性能而不是单纯的最小化代价函数。也即，代价函数最小时，模型性能不一定能达到理想要求。
希望可以计算出在数据生成分布基础上的优化值，但通常只知道样本集的经验分布，所以将优化作用在训练集上，最小化经验风险。但经验风险不一定有导数，例如分类问题，可能没办法最小化。则需要通过其他方法间接优化。

	1. 对于分类问题，通常选择优化代理损失函数(surrogate loss function)，如负对数似然或对数似然。提前终止也可以在过拟合之前停止优化。
	2. 批量和小批量优化


神经网络优化的挑战：

	1. 病态。Hessian矩阵病态，即使很小的更新步长也会增加代价函数。改进的牛顿求解法可以解决类似问题。
	2. 局部极小值。模型不可辨识性意味着神经网络代价函数具有非常多局部极小值，并且这些由于不可辨识性问题而产生的局部极小值都有相同的代价函数值。可以通过画出梯度范数随时间的变化来观察，如果梯度范数没有缩小到一个微小的值，那么该问题既不是局部极小值，也不是其他形式的临界点。
	3. 高原、鞍点和其他平坦区域。在鞍点处，Hessian矩阵同时具有正负特征值。Hessian矩阵在局部极小点处只有正特征值。局部极小点具有低代价的可能性比高代价要大得多，具有高代价的临界点更有可能是鞍点。二阶优化无鞍牛顿法
	4. 悬崖和梯度爆炸。遇到斜率极大的悬崖结构，可以使用启发式梯度截断。
	5. 长期依赖。变深的结构是模型丧失了学习到先前信息的能力。梯度消失与爆炸问题。
	6. 非精确梯度。大多数优化算法的先决条件是知道精确的梯度或是Hessian矩阵，但实际上通常会有噪声或者是有偏估计。可以选择比真实损失函数更容易估计的代理损失函数来避免。
	7. 局部和全局结构间弱对应。在全局结构中寻找可以找到更低代价的局部区域。
	8. 优化的理论限制。一些理论结果仅适用于神经网络单元输出离散值的情况。同时也存在不可解的问题，也很难判断一个特定问题是否属于该类。



优化算法：

	1. 随机梯度下降(SGD)。需要动态调整学习率，一般会线性衰减学习率直到完成指定次数的迭代，自后使学习率保持常数。也可以小批量梯度下降。
	2. 动量方法。目的是加速学习，处理高曲率、小且一致、或者带噪声的梯度。解决两个问题，Hessian矩阵病态条件和随机梯度的方差。
	3. Nesterov动量方法。


初始化参数的选择问题，到目前为止，初始化参数还只能拍脑袋完成， 然后再根据计算情况调整，几乎没有提供如何选择初始点的任何指导。
启发式方法可以选择权重初始大小：

	1. 一种方法是初始化m个输入n个输出的全连接层权重启发式方法是从均匀分布中采样。
	2. 另一种建议使用标准初始化


	1. 另外一种初始化推荐随机正交矩阵，并且仔细挑选负责每层非线性缩放或增益因子，得到用于不同类型的非线性激活函数的特定缩放因子。这种方案保证达到收敛所需的训练迭代总数独立于深度。
	2. 稀疏初始化。每个单元初始化为恰好有k个非零权重。该想法保持该单元输入的总数量独立于输入数目m，而不使单一权重元素的大小随m缩小。


偏置的设置方法必须和设置权重相协调，大部分情况下可以直接使用零偏置，但在如下情况中，需要设置偏置：

	1. 偏置作为输出单元，则初始化偏置以获取正确的输出边缘统计。设置偏置可以适用于分类器，也适用于自编码器和玻尔兹曼机。
	2. 需要选择偏置以避免初始化引起饱和。例如可以设置ReLU的隐藏单元为0.1。
	3. 一个单元会控制其他单元能否参与到等式中。例如LSTM模型需要设置遗忘门偏置为1。

除了初始化模型参数的简单常数或随机方法，还有可能使用机器学习初始化模型参数，可以使用相同的输入数据集，用无监督模型训练出来的参数来初始化监督模型。

自适应学习率算法：

	1. Delta-bar-delta，适用于全批量优化，如果损失对于摸个给定模型参数的偏导保持相同的符号，应增加学习率。如果偏导变化了符号，学习率应减小。
	2. AdaGrad。独立适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。只适合部分深度学习场景。应用于凸问题时快速收敛，当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同结构最终到达一个局部是凸碗的区域，该算法可能使学习率在达到这样的凸结构之前就变得很小。
	3. RMSProp。AdaGrad改进版本，在非凸设定下效果更好。改变梯度累积为指数加权移动平均，以此丢弃遥远过去的历史，使其可以找到凸碗结构后快速收敛。引入新的超参数来控制移动平均的窗宽。
	4. Adam。结合RMSProp和一些动量的变种。可以被认为对超参数的选择相当稳定。


目前比较流行的且使用较多的算法包括SGD、带动量的SGD、RMAProp、带动量的RMSProp、AdaDelta和Adam。没有确定的证据可以表明哪些算法更好，选择时多是根据任务的条件和对算法的熟悉程度。

二阶近似方法，在选定点做拟合曲线的二阶泰勒展开，而不是做一阶线性估计。求解方法：

	1. 牛顿法。需要Hessian矩阵保持正定，如果不为正定，则需要正则化，使得矩阵的负特征值相对接近零。即在Hessian矩阵对角线上加上正则化项，以抵消负特征值。这样可以避免在鞍点附近选择错误的梯度下降方向。该方法存在显著的计算负担，只有参数很少的网络才能使用牛顿法。
	2. 共轭梯度。用于有效避免Hessian矩阵求逆。在指定的搜索方向上找到了极小值后，在下一个搜索方向上继续搜索时可以保持上一个方向上的极小值，即不会造成重新搜索上一个方向的极小值。Fletcher-Reeves方法和Polak-Ribiere方法。非线性共轭梯度、缩放共轭梯度算法。
	3. BFGS算法。近似计算牛顿法中的Hessian矩阵逆。但必须存储Hessian逆矩阵，不适合现代百万级参数的深度学习模型。L-BFGS，避免存储完整的Hessian逆近似。


优化策略和元算法：

	1. 批标准化。一个自适应的重参数化方法，试图解决训练非常深的模型的困难。深度模型的每一层都有参数矩阵，每一层的参数矩阵都会对后面层产生影响，层数越深影响越大，则不容易选择合适的学习率，使得每一层都有合适的权重更新。批标准化方法的目的是假设参数矩阵中每个单元均服从高斯分布，每一层通过计算后得到的参数矩阵都会在分布上发生偏移，则通过标准化来使发生偏移的高斯分布重新变换为标准高斯分布，以此来消除每一层对后面层的影响。有操作会在标准化后再加上另外的参数。旧参数取决于其他层的复杂关联，标准化后新加入的参数仅由自身确定，与其他层级无关。(Ioffe and Szegedy, 2015)
	2. 坐标下降。在某些情况下，将一个优化问题分解为几个部分，单独对不同的部分做优化，反复循环所有变量，然后达到极小值。块坐标下降是指对于某个子集的变量同时最小化。只有在各个部分相对独立的情况下，或者更严格的讲相互正交的情况下才适合使用。
	3. Polyak平均。平均优化算法在参数空间访问轨迹中的几个点，然后选择下一个点。也可以选择指数衰减平均。
	4. 监督预训练。在问题十分困难，直接训练难以解决特定任务的情况下，训练一个简单的模型来求解问题，然后再提高模型的复杂度。贪心监督预训练，将任务分解为不同的子任务，对子任务分别做训练，然后再集成起来进一步训练。FitNets，先训练深度足够低宽度足够大容易训练的网络，在这个网络基础上继续训练更深更窄的网络。
	5. 从设计上选择有助于优化的模型。
	6. 延拓法和课程学习。延拓法是一族通过挑选初始点使优化更容易的方法，以确保局部优化花费大部分时间在表现良好的空间。课程学习方法先学习简单概念，然后逐步学习依赖于这些简化概念的复杂概念。



使用卷积处理可变尺寸输入，图像大小相对于卷积核来说，不是一个必须考虑的对象，如果要对每个像素生成标签，则不用考虑各个不同图像大小的情况，直接卷积即可。如果需要对整个图像生成标签，则需要统一输出大小，可以用池化解决。


卷积等效于使用傅里叶变换将输入与核都转换到频域、执行两个信号的煮点相乘，再使用傅里叶逆变换转换回时域。？？？设计更快的卷积计算方法而不损害模型准确性。其中一种方法是找到一个可分离的d维核，将该核表示成d个向量的外积，每一维一个向量，而后分别计算每一个维度的卷积。

减少卷积网络训练成本的一种方式是使用不是由监督方式训练得到的特征。有三种基本策略可以不通过监督训练而得到卷积核：

	1. 简单随机初始化
	2. 手动初始化
	3. 无监督来学习核。Coates et al. (2011) 将k 均值聚类算法应用于小图像块，然后使用每个学得的中心作为卷积核。

使用无监督的标准来学习特征，使得它们能够与位于网络结构顶层的分类层相互独立地确定。然后只需提取一次全部训练集的特征，构造
用于最后一层的新训练集。假设最后一层类似逻辑回归或者SVM，那么学习最后一层通常是凸优化问题。
学习特征，在每一层上单独学习特征，学习完成后再作为后一层的输入，这种方法不需要各层之间计算前向和反向传播。卷积模型的贪心逐层预训练的经典模型是卷积深度信念网络。
也可以用k值聚类先训练一小块模型而不是一层，则可以用这一小块模型的参数来定义卷积层的核。意味着使用无监督学习来训练卷积网络并且在训练过程中完全不用卷积是可能的。这种方法在计算能力低下的时代可以很好的降低训练成本。


循环网络如果采用隐藏节点到隐藏节点的连接，可以最大限度的保留历史信息，因此能够有强的适应性。但这种结构需要很大的计算能力来逐步计算每个时间点的状态，并且不能并行计算，训练代价很大。所以有另外简化的选择：

	1. 导师驱动模型。消除隐藏到隐藏循环，将上一节点的输出作为当前节点的输入，而不是使用上一节点的隐藏作为输入。优点是任何基于比较时刻t 的预测和时刻t 的训练目标的损失函数中的所有时间步都解耦了。因此训练可以并行化，即在各时刻t 分别计算梯度。因为训练集提供输出的理想值，所以没有必要先计算前一时刻的输出。该方式不再使用极大似然估计计算损失函数，而是在每个节点上使用真实值作为输入。这种办法的缺点是，每个节点的输出和真实值会有大的差距，这种情况下没法正确计算出结果。
	2. 采用的缓解方法可能是在一定的步长范围内保留隐藏到隐藏的连接，而在两个步长间使用结果输出，使相邻两步计算变为相邻两块计算。
	3. 另外一种方法是随机选择真实值集合中的一个作为输入，而不直接使用上一步的真实值，以此人为制造误差。



和前馈网络类似，循环网络可以使用任何损失函数，但是必须根据任务来选择合适的损失。

循环网络中使用参数共享的前提是相同参数可用于不同时间步的假设，即不同时刻的变量条件概率分布是平稳的。但也可以考虑在某个时间段内是满足平稳的，不同的时间段有不同的条件概率分布，这种判断比每个时刻都使用不同条件概率分布要简单很多，但网络必须在新的时刻开始时判断是否还属于当前的平稳时间段。

序列长度需要确定，可以采用方法：

	1. 加入一个对应于序列末端的特殊符号，产生该符号时，采样停止。训练集中，将该符号作为序列的一个额外成员，紧跟每个训练样本之后。
	2. 引入一个额外的伯努利输出，表示在每个时间步决定继续生成或停止生成。
	3. 将一个额外输出加入到模型中并且预测序列长度本身。模型可以采出t 的值，然后采t 步有价值的数据。这种方法需要在每个时间步的循环更新中增加一个额外输入，使得循环更新知道它是否是靠近所产生序列的末尾。这种额外的输入可以是t 的值，也可以是t - t即剩下时间步的数量。直接预测_ 的例子见Goodfellow et al. (2014d)


RNN可以完成如下场景的工作：

	1. 根据历史输出推断未来输出
	2. 根据历史输出和单个作用在所有历史时刻节点上的输入来推断未来输出。
	3. 根据历史输出和每个历史时刻上的不同输入来推断未来输出
	4. 根据历史和未来的输出和每个时刻上的不同输入来推断当前输出。


或者：

	1. 将输入序列映射成固定大小的向量
	2. 将固定大小的向量映射成输出序列
	3. 将一个输入序列映射到等长的输出序列
	4. 将输入序列映射到不等长输出序列


基于编码-解码的序列到序列架构：

	1. 编码器（encoder）或读取器(reader) 或输入(input) RNN 处理输入序列。编码器输出上下文C（通常是最终隐藏状态的简单函数）
	2. 解码器（decoder）或写入器(writer) 或输出(output) RNN 则以固定长度的向量（如图10.9 ）为条件产生输出序列
	3. 编码器RNN 的最后一个状态hnx 通常被当作输入的表示C 并作为解码器RNN 的输入。
	4. 此架构的一个明显不足是，编码器RNN 输出的上下文C 的维度太小而难以适当地概括一个长序列。这种现象由Bahdanau et al. (2015) 在机器翻译中观察到。他们提出让C 成为可变长度的序列，而不是一个固定大小的向量。此外，他们还引入了将序列C 的元素和输出序列的元素相关联的注意力机制（attention mechanism）。


大多数RNN的计算可以分解为三块参数及其相关变换：

	1. 从输入到隐藏
	2. 从前一隐藏到下一隐藏
	3. 从隐藏到输出

深度网络对RNN会有积极影响，但增加深度可能会因为优化困难而损害学习效果。三种增加深度的方式：

	1. 隐藏循环状态可以被分解为具有层次的组
	2. 可以向输入到隐藏，隐藏到隐藏以及隐藏到输出的部分引入更深的计算，可以延长链接不同时间步的最短路径
	3. 可以引入跳跃连接来缓解路径延长效应。


非常深的前馈网络通过精心设计的比例可以避免梯度消失和爆炸问题。有人可能会希望通过简单地停留在梯度不消失或爆炸的参数空间来避免这个问题。不幸的是，为了储存记忆并对小扰动具有鲁棒性，RNN 必须进入参数空间中的梯度消失区域(Bengio et al., 1993, 1994a)。实践中，Bengio et al. (1994a) 的实验表明，当我们增加了需要捕获的依赖关系的跨度，基于梯度的优化变得越来越困难，SGD 在长度仅为10 或20 的序列上成功训练传统RNN 的概率迅速变为0。

解决长期依赖的方法：

	1. 回声状态网络ESN和流体状态机。流体状态机使用脉冲神经元（二值输出）而不是ESN中的连续隐藏单元。ESN 和流体状态机都被称为储层计算（reservoir computing）(Lukoševičius and Jaeger, 2009)，因为隐藏单元形成了可能捕获输入历史不同方面的临时特征池。循环网络的一个重要特征就是Jacobian 矩阵的特征值谱J(t) =@s(t)@s(t��1) 。特别重要的是J(t) 的谱半径（spectral radius），定义为特征值的最大绝对值。最近，已经有研究表明，用于设置ESN 权重的技术可以用来初始化完全可训练的循环网络的权重（通过时间反向传播来训练隐藏到隐藏的循环权重），帮助学习长期依赖(Sutskever, 2012; Sutskever et al., 2013)。在这种设定下，结合第8.4 节中稀疏初始化的方案，设置1:2 的初始谱半径表现不错。
	2. 处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能把遥远过去的信息更有效地传递过来。存在多种同时构建粗细时间尺度的策略。这些策略包括在时间轴增加跳跃连接，“渗漏单元’’ 使用不同时间常数整合信号，并去除一些用于建模细粒度时间尺度的连接。
	3. 获得导数乘积接近1 的另一方式是设置线性自连接单元，并且这些连接的权重接近1。线性自连接的隐藏单元可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元（leaky unit）。我们可以通过两种基本策略设置渗漏单元使用的时间常数。一种策略是手动将其固定为常数，例如在初始化时从某些分布采样它们的值。另一种策略是使时间常数成为自由变量，并学习出来。在不同时间尺度使用这样的渗漏单元似乎能帮助学习长期依赖(Mozer, 1992; Pascanu et al., 2013a)。
	4. 处理长期依赖另一种方法是在多个时间尺度组织RNN 状态的想法(El Hihi and Bengio, 1996)，信息在较慢的时间尺度上更容易长距离流动。这个想法与之前讨论的时间维度上的跳跃连接不同，因为它涉及主动删除长度为一的连接并用更长的连接替换它们。以这种方式修改的单元被迫在长时间尺度上运作。而通过时间跳跃连接是添加边。收到这种新连接的单元，可以学习在长时间尺度上运作，但也可以选择专注于自己其他的短期连接。强制一组循环单元在不同时间尺度上运作有不同的方式。一种选择是使循环单元变成渗漏单元，但不同的单元组关联不同的固定时间尺度。这由Mozer (1992)提出，并被成功应用于Pascanu et al. (2013a)。另一种选择是使显式且离散的更新发生在不同的时间，不同的单元组有不同的频率。这是El Hihi and Bengio (1996)和Koutnik et al. (2014) 的方法。它在一些基准数据集上表现不错。
	5. 实际应用中最有效的序列模型称为门控RNN（gated RNN）。包括基于长短期记忆（long short-term memory）和基于门控循环单元（gatedrecurrent unit）的网络。
	6. 门控循环单元或GRU (Cho et al., 2014c; Chung et al., 2014, 2015a; Jozefowicz et al., 2015;Chrupala et al., 2015)。与LSTM 的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元的决定。
	7. 由Martens and Sutskever (2011) 提出了一个有趣的想法是，二阶导数可能在一阶导数消失的同时消失。二阶优化算法可以大致被理解为将一阶导数除以二阶导数（在更高维数，由梯度乘以Hessian 的逆）。如果二阶导数与一阶导数以类似的速率收缩，那么一阶和二阶导数的比率可保持相对恒定。
	8. 截断梯度（clipping the gradient）。此想法有不同实例(Mikolov, 2012; Pascanu et al., 2013a)。一种选择是在参数更新之前，逐元素地截断小批量产生的参数梯度(Mikolov, 2012)。另一种是在参数更新之前截断梯度g 的范数∥g∥ (Pascanu et al., 2013a)。梯度截断有助于处理爆炸的梯度，但它无助于消失的梯度。另一个想法是正则化或约束参数，以引导‘‘信息流’’。特别是即使损失函数只对序列尾部的输出作惩罚，我们也希望梯度向量∇h(t)L 在反向传播时能维持其幅度。


神经网络擅长存储隐性知识，但是他们很难记住事实。被存储在神经网络参数中之前，随机梯度下降需要多次提供相同的输入，即使如此，该输入也不会被特别精确地存储。

Weston et al. (2014) 引入了记忆网络（memory network），其中包括一组可以通过寻址机制来访问的记忆单元。记忆网络原本需要监督信号
指示他们如何使用自己的记忆单元。Graves et al. (2014) 引入的神经网络图灵机（neural Turing machine），不需要明确的监督指示采取哪些行动而能学习从记忆单元读写任意内容，并通过使用基于内容的软注意机制（ 见Bahdanau et al. (2015)和第12.4.5.1 节），允许端到端的训练。这种软寻址机制已成为其他允许基于梯度优化的模拟算法机制的相关架构的标准(Sukhbaatar et al., 2015; Joulin and Mikolov, 2015; Kumar et al., 2015a; Vinyals et al., 2015a; Grefenstette et al., 2015)。

产生确切整数地址的函数很难优化。为了缓解这一问题，NTM 实际同时从多个记忆单元写入或读取。读取时，它们采取许多单元的加权平均值。写入时，他们对多个单元修改不同的数值。用于这些操作的系数被选择为集中在一个小数目的单元，如通过softmax 函数产生它们。使用这些具有非零导数的权重允许函数控制访问存储器，从而能使用梯度下降法优化。

来自Ng (2015)。建议参考以下几个实践设计流程：
• 确定目标——使用什么样的误差度量，并为此误差度量指定目标值。这些目标和误差度量取决于该应用旨在解决的问题。
• 尽快建立一个端到端的的工作流程，包括估计合适的性能度量。
• 搭建系统，并确定性能瓶颈。检查哪个部分的性能差于预期，以及是否是因为过拟合、欠拟合，或者数据或软件缺陷造成的。
• 根据具体观察反复地进行增量式的改动，如收集新数据、调整超参数或改进算法。

实现步骤：

	1. 性能度量。确定目标（决定使用哪一种误差度量）。通常可以根据先前工具的基准结果来估计预期错误率。对不同的任务选取合适的度量对象，例如精度和召回率。算法需要能够估计判断的置信度，在置信度以外的不做判断。这些度量不是指泛化误差。

度量对象：
a. 精度 precision,，模型报告检测正确的比率
b. 召回率 recall，真实事件被检测到的比率
c. 覆盖 coverage，机器学习系统能够产生响应的样本所占的比率

AI-完全？


	1. 选择合适的模型和算法。如果简单，可以不选择深度学习模型而直接使用简单的抽象方法。

可选模型：
a. 全连接前馈网络
b. 卷积网络
c. 循环网络

可选非线性单元：
a. ReLU及其扩展
b. sigmoid
c. maxout
......

可选优化算法：
a. SGD及其扩展（具有衰减学习率和动量的SGD）
b. 批标准化
c. 正则化
d. Dropout
根据数据的结构选择一类合适的模型。如果项目是以固定大小的向量作为输入的监督学习，那么可以使用全连接的前馈网络。如果输入有已知的拓扑结构（例如，输入是图像），那么可以使用卷积网络。在这些情况下，刚开始可以使用某些分段线性单元（ReLU 或者其扩展，如Leaky ReLU、PReLU 和maxout）。如果输入或输出是一个序列，可以使用门控循环网络（LSTM或GRU）。

如果我们的任务和另一个被广泛研究的任务相似，那么通过复制先前研究中已知性能良好的模型和算法，可能会得到很好的效果。甚至可以从该任务中复制一个训练好的模型。例如，通常会使用在ImageNet 上训练好的卷积网络的特征来解决其他计算机视觉任务(Girshick et al., 2015)。


	1. 决定是否收集更多数据

首先，确定训练集上的性能是否可接受。如果模型在训练集上的性能就很差，学习算法都不能在训练集上学习出良好的模型，那么就没必要收集更多的数据。反之，可以尝试增加更多的网络层或每层增加更多的隐藏单元，以增加模型的规模。此外，也可以尝试调整学习率等超参数的措施来改进学习算法。如果更大的模型和仔细调试的优化算法效果不佳，那么问题可能源自训练数据的质量。数据可能含太多噪声，或是可能不包含预测输出所需的正确输入。这意味着我们需要重新开始，收集更干净的数据或是收集特征更丰富的数据集。
如果训练集上的性能是可接受的，那么我们开始度量测试集上的性能。如果测试集上的性能也是可以接受的，那么就顺利完成了。如果测试集上的性能比训练集的要差得多，那么收集更多的数据是最有效的解决方案之一。这时主要的考虑是收集更多数据的代价和可行性，其他方法降低测试误差的代价和可行性，和增加数据数量能否显著提升测试集性能。
收集更多的数据可能代价很高或者不可行。一个可以替代的简单方法是降低模型大小或是改进正则化（调整超参数，如权重衰减系数，或是加入正则化策略，如Dropout）。如果调整正则化超参数后，训练集性能和测试集性能之间的差距还是不可接受，那么收集更多的数据是可取的。
在决定是否收集更多的数据时，也需要确定收集多少数据。通常，加入总数目一小部分的样本不会对泛化误差产生显著的影响。因此，建议在对数尺度上考虑训练集的大小，例如在后续的实验中倍增样本数目。


	1. 选择超参数

a. 手动选择
手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。自动选择超参数算法大大减少了解这些想法的需要，但它们往往需要更高的计算成本。
手动设置超参数，我们必须了解超参数、训练误差、泛化误差和计算资源（内存和运行时间）之间的关系。手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化误差。手动搜索超参数的主要目标是调整模型的有效容量以匹配任务的复杂性。
有效容量受限于模型的表示容量、学习算法成功最小化训练模型代价函数的能力以及代价函数和训练过程正则化模型的程度。具有更多网络层，每层有更多隐藏单元的模型具有较高的表示能力——能够表示更复杂的函数。然而，如果训练算法不能找到某个合适的函数来最小化训练代价，或是正则化项（如权重衰减）排除了这些合适的函数，那么即使模型的表达能力较高，也不能学习出合适的函数。
调整学习率外的其他参数时，需要同时监测训练误差和测试误差，以判断模型是否过拟合或欠拟合，然后适当调整其容量。如果训练集错误率大于目标错误率，那么只能增加模型容量以改进模型。如果没有使用正则化，并且确信优化算法正确运行，那么有必要添加更多的网络层或隐藏单元。然而，令人遗憾的是，这增加了模型的计算代价。
如果测试集错误率大于目标错误率，那么可以采取两个方法。测试误差是训练误差和测试误差之间差距与训练误差的总和。寻找最佳的测试误差需要权衡这些数值。当训练误差较小（因此容量较大），测试误差主要取决于训练误差和测试误差之间的差距时，通常神经网络效果最好。此时目标是缩小这一差距，使训练误差的增长速率不快于差距减小的速率。要减少这个差距，我们可以改变正则化超参数，以减少有效的模型容量，如添加Dropout 或权重衰减策略。通常，最佳性能来自正则化得很好的大规模模型，比如使用Dropout 的神经网络。

b. 自动选择
原则上有可能开发出封装学习算法的超参数优化（hyperparameter optimization）算法，并选择其超参数，从而使用者不需要指定学习算法的超参数。令人遗憾的是，超参数优化算法往往有自己的超参数，如学习算法的每个超参数应该被探索的值的范围。
当有三个或更少的超参数时，常见的超参数搜索方法是网格搜索（grid search）。对于每个超参数，使用者选择一个较小的有限值集去探索。通常，网格搜索大约会在对数尺度（logarithmic scale）下挑选合适的值，例如，一个学习率的取值集合是f0:1; 0:01; 10��3; 10��4; 10��5g，或者隐藏单元数目的取值集合f50; 100; 200; 500; 1000; 2000g。

随机搜索过程：
为每个超参数定义一个边缘分布，与网格搜索不同，不需要离散化超参数的值与网格搜索一样，我们通常会重复运行不同版本的随机搜索，以基于前一次运行的结果改进下一次搜索。

随机搜索能比网格搜索更快地找到良好超参数的原因是，没有浪费的实验，不像网格搜索有时会对一个超参数的两个不同值（给定其他超参数值不变）给出相同结果。在网格搜索中，其他超参数将在这两次实验中拥有相同的值，而在随机搜索中，它们通常会具有不同的值。

关于超参数优化的最前沿方法还包括Spearmint (Snoek et al., 2012)，
TPE (Bergstra et al., 2011) 和SMAC (Hutter et al., 2011)。



	1. 调试策略

可以设计一种足够简单的情况，能够提前得到正确结果，判断模型预测是否与之相符；我们也可以设计一个测试，独立检查神经网络实现的各个部分。一些重要的调试检测如下所列。
a. 可视化计算中模型的行为：当训练模型检测图像中的对象时，查看一些模型检测到部分重叠的图像。在训练语音生成模型时，试听一些生成的语音样本。也即直接观察模型每一层的中间结果输出，看是否和预期相符。
b. 可视化最严重的错误：大多数模型能够输出运行任务时的某种置信度量。例如，基于softmax 函数输出层的分类器给每个类分配一个概率???
c. 根据训练和测试误差检测软件：如果训练误差较低，但是测试误差较高，那么很有可能训练过程是在正常运行，但模型由于算法原因过拟合了。另一种可能是，测试误差没有被正确地度量，可能是由于训练后保存模型再重载去度量测试集时出现问题，或者是因为测试数据和训练数据预处理的方式不同。如果训练和测试误差都很高，那么很难确定是软件错误，还是由于算法原因模型欠拟合。
d. 拟合极小的数据集：当训练集上有很大的误差时，我们需要确定问题是真正的欠拟合，还是软件错误。通常，如果不能训练一个分类器来正确标注一个单独的样本，或不能训练一个自编码器来成功地精准再现一个单独的样本，或不能训练一个生成模型来一致地生成一个单独的样本，那么很有可能是由于软件错误阻止训练集上的成功优化。此测试可以扩展到只有少量样本的小数据集上。
e. 比较反向传播导数和数值导数：验证这些求导正确性的一种方法是比较实现的自动求导和通过有限差分（finite difference）计算的导数。可以使用中心差分（centered difference）提高近似的准确率。扰动大小ϵ 必须足够大，以确保该扰动不会由于数值计算的有限精度问题产生舍入误差。如果可以在复数上进行数值计算，那么使用复数作为函数的输入会有非常高效的数值方法估算梯度(Squire and Trapp, 1998)。
f. 监控激活函数值和梯度的直方图：可视化神经网络在大量训练迭代后（也许是一个轮）收集到的激活函数值和梯度的统计量往往是有用的。隐藏单元的预激活值可以告诉我们该单元是否饱和，或者它们饱和的频率如何。




通过设计一些特定的CPU 上的操作可以大大提升效率。除了选择定点运算或者浮点运算以外，其他的策略还包括了如通过优化数据结构避免高速缓存缺失、使用向量指令等。

使用多个机器并行地计算多个梯度下降步骤是一个更好的选择。不幸的是，梯度下降的标准定义完全是一个串行的过程：第t 步的梯度是第t �� 1 步所得参数的函数。
这个问题可以使用异步随机梯度下降（Asynchoronous Stochastic GradientDescent）(Bengio and Bengio, 1996; Recht et al., 2011) 解决。在这个方法中，几个处理器的核共用存有参数的内存。每一个核在无锁情况下读取这些参数并计算对应的梯度，然后在无锁状态下更新这些参数。由于一些核把其他的核所更新的参数覆盖了，因此这种方法减少了每一步梯度下降所获得的平均提升。但因为更新步数的速率增加，总体上还是加快了学习过程。Dean et al. (2012) 率先提出了多机器无锁的梯度下降方法，其中参数是由参数服务器（parameter server）管理而非存储在共用的内存中。分布式的异步梯度下降方法保留了训练深度神经网络的基本策略，并被工业界很多机器学习组所使用(Chilimbi et al., 2014; Wu et al., 2015)。学术界的深度学习研究者们通常无法负担那么大规模的分布式学习系统，但是一些研究仍关注于如何在校园环境中使用相对廉价的硬件系统构造分布式网络(Coates et al., 2013)。


在许多商业应用的机器学习模型中，一个时间和内存开销较小的推断算法比一个时间和内存开销较小的训练算法要更为重要。减少推断所需开销的一个关键策略是模型压缩（model compression） (Buciluˇa
et al., 2006)。模型压缩的基本思想是用一个更小的模型取代替原始耗时的模型，从而使得用来存储与评估所需的内存与运行时间更少。


加速数据处理系统的一种策略是构造一个系统，这个系统用动态
结构（dynamic structure）描述图中处理输入的所需计算过程。在给定一个输入的情况中，数据处理系统可以动态地决定运行神经网络系统的哪一部分。单个神经网络内部同样也存在动态结构，给定输入信息，决定特征（隐藏单元）哪一部分用于计算。这种神经网络中的动态结构有时被称为条件计算（conditional computation）(Bengio et al., 2013b,c)。由于模型结构许多部分可能只跟输入的一小部分有关，只
计算那些需要的特征可以起到加速的目的。

在分类器中加速推断的可行策略是使用级联（cascade）的分类器。当目标是检测罕见对象（或事件）是否存在时，可以应用级联策略。要确定对象是否存在，我们必须使用具有高容量、运行成本高的复杂分类器。

决策树本身是动态结构的一个例子，因为树中的每个节点决定应该使用哪个子树来评估输入。一个结合深度学习和动态结构的简单方法是训练一个决策树，其中每个节点使用神经网络做出决策(Guo and Gelfand, 1992)
可以使用称为选通器（gater）的神经网络来选择在给定当前输入
的情况下将使用几个专家网络（expert network）中的哪一个来计算输出。这个想法的第一个版本被称为专家混合体（mixture of experts）(Nowlan, 1990; Jacobs et al.,1991)，其中选通器为每个专家输出一个概率或权重（通过非线性的softmax 函数获得），并且最终输出由各个专家输出的加权组合获得。

Bengio et al. (2013c) 提出使用选通器概率梯度的若干估计器，而Bacon et al. (2015); Bengio et al. (2015a) 使用强化学习技术（ 策略梯度（policy gradient））来学习一种条件的Dropout 形式（作用于隐藏单元块），减少了实际的计算成本，而不会对近似的质量产生负面影响。

另一种动态结构是开关，其中隐藏单元可以根据具体情况从不同单元接收输入。这种动态路由方法可以理解为注意力机制（attention mechanism）(Olshausenet al., 1993)。目前为止，硬性开关的使用在大规模应用中还没有被证明是有效的。


计算机视觉方向的应用：
一些卷积模型接受可变大小的输入并动态地调整它们的池化区域大小以保持输出大小恒定(Waibel et al., 1989)。其他卷积模型具有可变大小的输出，其尺寸随输入自动缩放，例如对图像中的每个像素进行去噪或标注的模型(Hadsell et al., 2007)。

	1. 预处理

数据集增强可以被看作是一种只对训练集做预处理的方式。数据集增强是减少大多数计算机视觉模型泛化误差的一种极好方法。
a. 对比度归一化
全局对比度归一化：
在深度学习中，对比度通常指的是图像或图像区域中像素的标准差。
全局对比度归一化（Global contrast normalization, GCN）旨在通过从每个图像中减去其平均值，然后重新缩放其使得其像素上的标准差等于某个常数s 来防止图像具有变化的对比度。引入一个小的正的正则化参数_ 来平衡估计的标准差。或者，我们至少可以约束分母使其大于等于ϵ。
通过设置_ = 0 来忽略小分母问题是安全的，并且在非常罕见的情况下为了避免除以0，通过将ϵ 设置为一个非常小的值比如说10��8。这也
是Goodfellow et al. (2013c) 在CIFAR-10 数据集上所使用的方法。随机剪裁的小图像更可能具有几乎恒定的强度，使得激进的正则化更有用。在处理从CIFAR-10 数据中随机选择的小区域时，Coates et al. (2011) 使用ϵ = 0; _ = 10。可以把GCN 理解成到球壳的一种映射。
存在被称为sphering（sphering）的预处理操作，并且它不同于GCN。sphering 并不会使数据位于球形壳上，而是将主成分重新缩放以具有相等方差，使得PCA 使用的多变量正态分布具有球形等高线。sphering 通常被称为白化（whitening）。
全局对比度归一化常常不能突出我们想要突出的图像特征，例如边缘和角。

局部对比度归一化：
局部对比度归一化（local contrast normalization, LCN）。局部对比
度归一化确保对比度在每个小窗口上被归一化，而不是作为整体在图像上被归一化。
局部对比度归一化通常可以通过使用可分离卷积（参考第9.8 节）来计算特征映射的局部平均值和局部标准差，然后在不同的特征映射上使用逐元素的减法和除法。
局部对比度归一化是可微分的操作，并且还可以作为一种非线性作用应用于网络隐藏层，以及应用于输入的预处理操作。
与全局对比度归一化一样，我们通常需要正则化局部对比度归一化来避免出现除以零的情况。事实上，因为局部对比度归一化通常作用于较小的窗口，所以正则化更加重要。较小的窗口更可能包含彼此几乎相同的值，因此更可能具有零标准差。

b. 数据集增强
很容易通过增加训练集的额外副本来增加训练集的大小，进而改进分类器的泛化能力。这些额外副本可以通过对原始图像进行一些变化来生成，但是并不改变其类别。对象识别这个分类任务特别适合于这种形式的数据集增强，因为类别信息对于许多变换是不变的，而我们可以简单地对输入应用诸多几何变换。
