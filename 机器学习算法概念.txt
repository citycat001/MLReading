机器学习算法是一种能够从数据中学习的算法。
机器学习模型是一种能够以特定的模式组织、分配、集成、使用机器学习算法的模型。

学习的定义：给定任务T和任务度量P，一个计算机程序可以通过经验E改进后在任务T上由P衡量的性能有所提升。

特征：一个可被计算机处理的对象或事件所特有的属性，该属性可以明确描述或表达对象或事件的某一维度。
样本：是指从某些希望机器学习系统处理的对象或事件中收集到的量化特征，也可称为数据点。数学表达为一个n维实向量，该向量中的每个元素表达一个特征。
数据集：样本的集合，可以用设计矩阵来表示。

经验E：由机器学习算法从数据集中获得的，可以用来优化计算机程序性能的信息。
任务T：人类指定计算机程序所需要完成的工作，该工作可以从指定数据集中获取经验E。
性能度量P：用以定量评估完成任务T的计算机程序性能的一组指标。

任务T类型：
分类
输入缺失分类
回归
转录
机器翻译
结构化输出
异常检测
合成和采样
缺失值填补
去噪
密度估计或概率质量函数估计

度量P包含的指标：
准确率：指完成任务T的计算机程序输出正确结果的样本比率。
错误率：指完成任务T的计算机程序输出错误结果的样本比率。

经验E类型：
监督：样本数据有对应的映射值。观察随机向量样本和对应的值或向量，从新样本预测对应值或向量。
无监督：样本数据没有对应的映射值。尝试从显式或隐式地学习出概率分布，或者是该分布的某些性质。
强化学习：学习系统及其训练过程有反馈回路。
映射值可称为标签或者目标。
无监督学习也可通过概率链式法则转换为监督学习。监督学习也可通过无监督学习策略学习联合分布，然后用贝叶斯公式计算监督学习结果。

泛化：在从未观测到的输入上表现良好的能力。
训练误差：经过训练后的模型在训练集上的计算结果和真实值之间的误差。
泛化(测试)误差：经过训练后的模型在测试集上的计算结果和真实值之间的误差。
欠拟合：不能在训练集上获得足够低的训练误差。
过拟合：训练误差和测试误差间差距太大。

容量：机器学习模型的容量是指该模型拟合各种函数的能力。在指定数据集上可拟合各种函数的能力越强(能够拟合更多种类的函数，能够以更高的精度拟合指定函数)，模型容量越大，但可能会过拟合；拟合各种函数越弱(可拟合的函数种类少，不能拟合或者只能以低精度拟合指定函数)，模型容量越小，可能会欠拟合。

假设空间：人为选定的以指定数据集为基础可能拟合出的函数集合。选择假设空间（根据经验或者没有经验拍脑袋决定可能拟合出的函数集）是一种控制模型容量的方法。这里可控制的模型容量可以认为是一种表示容量，或者是能够通过拟合假设空间中实际可以拟合的函数来间接体现模型能力。

表示容量：以指定模型为基础，可以预想拟合出假设空间中所有函数的能力。
有效容量：以指定模型为基础，可以实际拟合出假设空间中函数的能力。
训练误差和泛化误差之间差异的上界随着模型有效容量的增长而增长，但随着训练样本增多而下降。
最优容量：训练误差和泛化误差间差距最小时模型的容量。具有最优容量的模型仍然有可能在训练误差和泛化误差之间存在很大差距，此时可以通过更多的训练样本来缩小差距。

VC维度：一种量化模型容量的指标，定义为分类器能够分类的训练样本的最大数目。（只能用在分类任务中？）

非参数模型：不采用参数做拟合，而是根据样本本身已有数据来直接预测，例如最近邻接回归，根据算法计算输入数据附近所有邻近向量对应数值的平均值。在非参数模型中，更多的训练样本数目会得到更好的泛化能力。

贝叶斯误差

没有免费午餐定理：在所有可能的数据生成分布上平均后，每个分类模型在新观测的点上有相同的错误率，或，某种意义上说没有一个机器学习模型总是比其他的要好。现实中，如果在未知概率分布情况下假设概率分布，可以设计在这些分布上效果好的学习模型。也即，没有最优的通用模型可以适合所有数据集，只能针对特定的数据集寻找相对优秀的模型。

正则化：指修改学习算法，使其降低泛化误差的方法，也是在特定数据集中寻找相对优秀的模型的一种方法。

函数估计：在假设空间中，以指定数据集为基础估计数据集内的映射关系，此种估计为近似关系，和真实函数有误差。

均方误差MSE度量估计和真实参数之间平方误差的总体期望偏差。用MSE度量泛化误差时，偏差和方差对于泛化误差都有意义，增加容量会增加方差并降低偏差。

极大似然估计：计算在指定参数的条件下，给定数据集最大概率能拟合出的函数。
在如下条件下，极大似然估计具有一致性，或训练样本趋于无穷大时，参数的极大似然估计会收敛到参数的真实值：
真实分布必须在事先预想的可能的模型分布族中。（这里的模型是指数据集中样本的概率分布模型，不是机器学习模型，可以理解为，数据集可以拟合函数，但有随机噪声存在，拟合出的函数是通过样本概率分布得到期望值去噪之后得来的；或认为，所有的样本并不是正好落在拟合的函数曲线、曲面或超曲面上，而是在附近，拟合函数也不可能让函数曲线穿过所有样本点，这样会过拟合，因此确定函数曲线在样本附近的拟合值需要通过样本概率分布模型获得期望，用期望值作为拟合值，以此去除样本所带来的随机噪声并完成拟合。）
真实分布必须刚好对应指定类型参数。

线性回归方法：最小二乘法、局部加权线性回归、岭回归（即权重衰减正则化的最小二乘法）、Lasso、LAR、PCA、前向逐步回归（梯度下降算法？）。

贝叶斯统计：指定参数的先验概率分布，根据观察到的数据集不断的调整参数的概率分布，即在数据集基础上的条件概率分布，可用贝叶斯公式计算。

最大后验估计MAP：在贝叶斯统计计算中选择后验概率最大的点。
权重衰减正则化的极大似然学习可以被解释为贝叶斯推断的MAP近似。这个适应于正则化时加到目标函数的附加项对应着的是对数先验概率。

概率监督学习：使用极大似然估计找到对于有参分布最好的参数。

逻辑回归：线性分类算法。在使用极大似然估计完成线性回归之后，在因变量结果上使用Logistic Sigmoid函数将线性输出转换为值域(0, 1)，则可以用来分类。不同类别的概率和归一化为1，符合值域区间。

支持向量机：线性分类算法。使用极大似然估计完成线性回归，当因变量输出值为正时，预测为正类；当输出值为负时，预测为负类。
核技巧：将输入向量替换为特征函数输出，点积被替换为核函数。核技巧可以认为是使用特征函数将数据集预处理后，将数据集转换到新的空间中做线性回归。该技巧能够使用保证有效收敛的凸优化技术来学习非线性模型。
高斯核：径向基函数核
核机器/核方法：使用核技巧的算法。


前馈网络，由输入一次经过多个隐含层后得到最后输出，该过程一次完成，没有循环反馈。学习非线性映射时关键在于选择哪种激活函数。

平均绝对误差：在优化的函数族中的一个代价函数，这个函数可以对每个因变量预测自变量取值的中位数。

神经网络隐藏节点非线性变换种类：
整流线性单元（ReLU）。变种：绝对值整流（Absolute value rectification）、渗漏整流线性单元（Leakey ReLU）、参数化整流线性单元（parametric ReLU）、maxout单元
logistic sigmoid，双曲正切函数。选择双曲正切函数会更容易完成线性回归。sigmoid更适合循环网络。
softmax
径向基函数（RBF）
softplus
应双曲正切函数（hard tanh）

神经网络深度和宽度

梯度计算的反向传播方法和其他微分算法。反向传播只是反向模式累加的特殊情况。自动微分？


卷积网络：
卷积运算，人工智能领域的卷积运算与数学领域中不同，该运算是离散的矩阵元素乘积之和，不是实变函数的乘积。卷积运算通过三个重要思想来帮助改进机器学习系统：
稀疏交互：不使用全映射，而是使用核来提取部分映射。
参数共享：使用相同的核来映射不同的隐含层。
等变表示：对输入的平移变化得出的卷积结果相同。
卷积的目的是简化和抽象神经网络中相邻层之间的映射，以在保证相当程度的精度的同时提高运算效率，降低资源使用率。


卷积网络中的典型层包含三级：
并行卷积计算的线性激活响应
非线性激活（探测级）
池化

池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。主要作用是支持平移不变，这种性质可以支持只关心特征是否出现，而不关心它出现的位置。以图像为例，池化可以不考虑图片的大小，对不同大小的相似图片得出相同的结果。
一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导(Boureau et al., 2010)。将特征一起动态地池化也是可行的，例如，对于感兴趣特征的位置运行聚类算法(Boureau et al., 2011)。这种方法对于每幅图像产生一个不同的池化区域集合。另一种方法是先学习一个单独的池化结构，再应用到全部的图像中(Jia et al., 2012)。

卷积的平移等变性和池化的平移不变性！特征提取的理论基础。
把卷积神经网络当作一个具有无限强先验的全连接网络来实现会导致极大的计算浪费。但把卷积神经网络想成具有无限强先验的全连接网络可以帮助我们更好地洞察卷积神经网络是如何工作的。

使用零填充来确保图片边界像素能在深度网络中持续转播。三种选择：
没有填充。有效卷积（valid），不填充，则图像输出会在每一层缩减，并最终缩减为1x1。
填充使得输入和输出像素大小相同。相同卷积（same）。
填充使得每个像素在每个方向上都有相同的访问次数。全卷积（full）。
填充数量应在有效卷积和相同卷积之间。

局部连接（非共享卷积），和卷积类似的计算方法，但是每个局部连接映射使用不同的核。
平铺卷积，循环使用一组核而不是一个核作用于输入，是卷积和局部连接的综合。

局部连接层与平铺卷积层都和最大池化有一些有趣的关联：这些层的探测单元都是由不同的过滤器驱动的。如果这些过滤器能够学会探测相同隐含特征的不同变换形式，那么最大池化的单元对于学得的变换就具有不变性。卷积层对于平移具有内置的不变性。

从线性变换角度看卷积运算，卷积是线性变换，所以可以用矩阵乘法表示，将卷积核分解为另外一个矩阵及其转置矩阵的乘积，而后乘以输入张量得到卷积值。这种表示方式可以在反向传播误差时起到作用，便于计算反向传播。

转置运算返回的输出的大小取决于三个方面：零填充的策略、前向传播运算的步幅以及前向传播的输出映射的大小。？？

结构化输出？？不清楚这个的意义。但是内容有说明，这个结构化可以在卷积网络中集成循环网络，例如将图像张量分解为行、列、颜色通道三个方向，目标是输出标签张量（即分类结果），该输出遵循每个像素的标签的概率分布。循环网络使用标签张量的先验估计作为输入，通过图像张量三个不同方向上的迭代来逐步改善估计，每个方向上的估计都使用相同的卷积核参数。

出现卷积神经网络的原因：模拟哺乳动物视觉神经处理信息的方式。
初级视觉皮层可以进行空间映射
初级视觉皮层包含许多简单细胞，这些细胞可以认为是在一个小空间位置内的图像的线性函数。卷积网络的检测器单元被设计为模拟简单细胞的性质。
也包括复杂细胞。用来响应由简单细胞检测的那些特征，复杂细胞对于特征的位置微小偏移具有不变性，可以由池化来模拟。复杂细胞对于照明中的一些变化也是不变的，不能简单的用空间位置池化来刻画，需要一些跨通道池化策略，如maxout单元。
最后找到响应特定概念的细胞，这些细胞对输入的很多种变换都具有不变性。
研究者已经研发了几种具有视觉机制的视觉模型，但到目前为止还没有成为主导方法(Larochelle and Hinton, 2010; Denil et al., 2012)。

Gabor 函数（Gabor function）二维坐标函数，描述在二维空间点的对应权重。可以参考(Hyvarinen et al., 2009) 来获得自然图像统计领域的综述。

卷积网络专门用于处理网格化数据，循环网络则处理序列数据。

在1维时间序列上使用卷积是时延神经网络的基础。(Lang and Hinton, 1988; Waibel et al., 1989; Lang et al., 1990)

计算图是形式化一组计算结构的方式。
在研究循环网络时，展开过程引入两个主要优点：
无论序列长度，学成模型始终具有相同输入大小，因为它指定的是从一种状态到另外一种状态的转移，而不是在可变长度的历史状态上操作。
可以在每个时间步使用相同参数的相同转移函数。

循环网络的设计模式：
每个时间步都有输出，并且隐藏单元间有循环链接的循环网络
每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络
隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络

BPTT，循环网络中计算时序数据的反向误差传播。

双向循环网络用于解决当前节点的值的意义取决于未来节点的意义或者状态。需要未来的数据反向反馈。在需要双向信息的应用中非常成功(Graves, 2012)，如手写识别(Graves et al., 2008; Graves and Schmidhuber, 2009)，语音识别(Graves and Schmidhuber, 2005; Graves et al., 2013) 以及生物信息学(Baldi et al., 1999)。


递归神经网络是循环网络的另一个扩展，形态是深的树状结构而不是链状结构。递归网络已成功地应用于输入是数据结构的神经网络(Frasconi
et al., 1997, 1998)，如自然语言处理(Socher et al., 2011a,c, 2013a) 和计算机视觉 (Socher et al., 2011b)。递归网络的一个明显优势是，对于具有相同长度_ 的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从_ 减小为O(log _ )，这可能有助于解决长期依赖。由每个节
点执行的计算无须是传统的人工神经计算（所有输入的仿射变换后跟一个单调非线性）。例如，Socher et al. (2013a) 提出用张量运算和双线性形式。
